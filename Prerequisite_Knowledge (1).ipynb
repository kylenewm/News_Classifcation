{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisite Knowledge"
      ],
      "metadata": {
        "id": "jJKr8SnxukSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Natural Language Processing**\n",
        "We will be using the NLTK library for text-preprocessing. There are other libraries that may be faster such as spaCY. However, runtime should not be an issue since we are not working with very large amounts of data. Here is a link to NLTK documentation: https://www.nltk.org/\n",
        "\n",
        "NLP basics such as tokenization, lemmitization, chunking, stemming, etc. are shown in the documentation so I will not be going into detail about how each of them work. "
      ],
      "metadata": {
        "id": "0B46JuA1vdiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word Embeddings**\n",
        "\n",
        "Word embedding is the process of converting text into numbers. It is the same thing as vectorizing but is specifc to the context of NLP.\n",
        "\n",
        ">There are many different methods for word embeddding that are usually placed in two separate categories. I will not go too far into detail about every word embedding technique but will list them if you want to read into them further. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Count/Statisitcal word embeddings:**\n",
        "\n",
        "\n",
        ">  These include One-hot Encoding, Bag of words, N-grams vectorization (Count vectorization is the technique where n = 1, TFIDF vectorization,\n",
        "\n",
        "\n",
        "\n",
        "**Predictive word embeddings**:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> These include Word2vec, Fast text, Glove\n",
        "\n",
        "**It is important to note that predictive word embedding techniques will generally perform better than frequency-based techniques. However, I will be using a predictive word embedding tehcnique on a later project**\n",
        "\n",
        "\n",
        "Since this as a project for unsupervised learning, I will not being using this word embedding technique for the assignment.\n",
        "\n",
        ">To develop a deep understanding of the Natural Langauge Toolkit Library, I read through [Natural Language Processing w/ Python](https://www.nltk.org/book/)\n",
        "\n",
        "> For a stronger understanding of word embeddings, I would read the artice by [analytics vidhya](https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/)"
      ],
      "metadata": {
        "id": "BLebW_EluZjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TFIDF Vectorizer**"
      ],
      "metadata": {
        "id": "98pSmLKbvnJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This shows us that the vectorizer outputs a matrix with 1490 rows (articles) by 9927 columns (words). TfidfVectorizer takes a set of documents with words and assigns a numerical value signifiying it's importance.\n",
        "The math of tfidf is explained as follows:\n",
        "\n",
        ">The full equation is $tfidf(w,d,D) = tf(t,d) * idf(t,D) $\n",
        "Let's break this equation into two parts:\n",
        "Term frequency: $tf(t,d)$ and inverse document frequency: $idf(t,D)$\n",
        "\n",
        "1. \n",
        "The tf(term frequency) is the total count of the word within each document.\n",
        "The mathemical notation is:\n",
        "\n",
        "$$\\frac{f_t,d}{\\sum_{t^{'} \\in d}{{f_{t',d}}}}$$\n",
        "\n",
        ">>Here we can say that $tf(t,d)$ will ouput a matrix that is $m \\times n$ (m documents and n words) with the frequency of every word in every document. It is important that term frequncy can be defined differently. An couple examples would to be taking the boolean frequencies or logarithmically scaled frequencies. \n",
        "\n",
        "\n",
        "2. The idf (inverse document frequency) is a function that calculates the log of the total number of documents divided by the total number of documents a word appears in. Basically the more documents a word appears in, the lower the idf for that word. The mathematical notation is\n",
        "\n",
        "$$log \\frac{N}{\\vert \\{ d \\in D : t \\in d \\} \\vert} $$\n",
        "\n",
        ">>Here N is the total number of documents and $\\vert \\{ d \\in D : t \\in d \\} \\vert$ is the number of documents the term $t$ appears in\n",
        "The output of this is an $ 1 \\times n $ matrix (every word has 1 idf score)\n",
        "\n",
        "> Multiply the 2 together and we get a matrix that has scores for every word in every document. The normalization takes place under the hood and is beyond the scope of this tuotorial.\n"
      ],
      "metadata": {
        "id": "2sK6x8i5uebL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Metrics for Classification:**\n",
        "Fraud detection is an easy way to understand basic metrics. \n",
        "TP= True Posiitve\n",
        "FP = False Positive\n",
        "TN = True Negative\n",
        "FN = False Negative\n",
        "**accuracy** = $\\frac{TP + TN}{TP + FN + FP + FN}$\n",
        "> Simple explanation: how often the model makes the correct prediction \n",
        "\n",
        "> Ex. When the model makes a prediction of \"Fraud\" or \"No fraud\", how often is it correct?\n",
        "\n",
        "**precision**: $\\frac{TP}{TP + FP}$\n",
        "> Simple explanation: When the model predicts a positive, how often is there actually a positive?\n",
        "\n",
        "> Ex. When the model predicts fraud, how often is fraud occurring?\n",
        "\n",
        "**Recall**: $\\frac{TP}{TP + FN}$\n",
        "> Simple explanation: When there is a positive, how often are we detecting the posiitve\n",
        "\n",
        "> Ex. When there is fraud, how often does the model detect it?\n",
        "\n",
        "**f1_score** = $\\frac{TP + TN}{TP + FN + FP + FN}$\n",
        "\n",
        "Note: When there are more than two classes, we have to choose a way of averaging the metric. The basics are micro and macro:\n",
        "1. Macro-average: Every class contributes equally to the final outcome. So if class 1 has fewer samples than class 2, we will weight each prediction in class 1 more heavily than the predictions in class2. \n",
        "\n",
        "2. Micro-average: Every prediction is weighted the same regardless of class distribution. \n",
        "\n",
        "For a more in depth understanding: [scikit-learn metrics documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "GxutoTXGus7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Supervised Learning Algorithms**\n",
        "\n",
        "1. Random Forest Classifiers \n",
        "\n",
        "2. Support Vector Machines\n",
        "\n",
        "3. Logistic Regression\n",
        "\n",
        "4. Naive Bayes Classifier\n",
        "\n",
        "5. K-nearest neighbors"
      ],
      "metadata": {
        "id": "clbtHdgYvBJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** will not go into detail on the different algorithms mentioned above as there are numerous videos and articles explaining each of them in depth. To clear any confusion random forests expand on the concept of decision trees, so I would start there first. Also, logistic regression and naive bayes require an undersanding of probability of statistics and support vector machines require an understanding of linear algebra."
      ],
      "metadata": {
        "id": "BYc231ofus5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross Validation\n",
        "Cross validation eliminates the need of allocating part of the dataset for a validation set. A common form of cross validation is k-fold cross validation. This is where the data is split into k chunks where k-1 chunks are training data and the kth chunk is the test data. We can then average metrics across the different validation sets so we will have a much better understanding of the new model performs on new (not actually new) data. Statified k-fold cross validation uses stratified sampling (maintains consistency of original class) instead of random-sampling (as used in k-fold cross validation). For the project I will be using stratified 5-fold cross validation. \n",
        "\n",
        "For a more in depth understanding:\n",
        "[scikit-learn cross validation documentation](https://scikit-learn.org/stable/modules/cross_validation.html)"
      ],
      "metadata": {
        "id": "rIEZJWWXvBHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning and Optimization\n",
        "There are a variety of different possible hyperparameter optimization tuning methods such as a gridsearch, random search, halvingsearch, and so on. For my project, I will be using a randomized search with 3-fold cross validation (due to CPU constraints) as well as a grid search based off the best hyperparemters from the grid search. Due to processor constraints this will only be done on the pipleine consisting of the TFIDF vectorizer and Random Forest Classifier. "
      ],
      "metadata": {
        "id": "omLTQGtRx1sh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non Negative Matrix Factorization\n",
        "Non negative matrix factorization takes a vectorized matrix (we used the tfidf vectorizer) that is $n \\times m$ (words $\\times$ articles) and is  split into a W and H matrix. The W matrix is $n \\times k$ (words $\\times$ topics). The H matrix is $k \\times m$ (documents $\\times$ topics). The loss function is then minimized by recalculating the W and H matrix so the it as close as possible to the A matrix (vectorized matrix). Mathematically, this is where the weighted values converge. We can then extract the most important features form the W matrix as well as which documents fit which topics. For our purposes, we are interested in the H matrix to extract the predicted topic for each article. \n",
        "\n",
        "**Issue with NMF for classifcation** : Nonnegative matrix factorization is often utlized for topic modeling and not top classifcation. For topic modeling, we can choose a certain number of topics for the matrix to be contained in. For our purposes, we will choose 5 topics (since we have 5 different catgeories to classify articles). "
      ],
      "metadata": {
        "id": "iTeXaJ5cyGET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other knowledge needs:\n",
        "\n",
        "\n",
        "1.   Probability and Stastistics\n",
        "2.   Linear Algebra\n",
        "3. Understanding of dependencies \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hz82RFCovBBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Links to useful tutorials/libraries used:\n",
        "1. https://www.analyticsvidhya.com/blog/2022/01/roadmap-to-master-nlp-in-2022/\n",
        "2. https://www.nltk.org/\n",
        "3. https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
        "4. https://www.w3schools.com/python/python_regex.asp"
      ],
      "metadata": {
        "id": "cXBD-dgJusxa"
      }
    }
  ]
}